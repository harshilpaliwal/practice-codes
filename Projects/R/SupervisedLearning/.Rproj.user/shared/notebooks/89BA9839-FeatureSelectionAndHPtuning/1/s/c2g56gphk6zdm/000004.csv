"0","#The above method was pretty manual and hence time taking and inefficient."
"0","#We will now use glmnet and LASSO penalty to improve and quicken the results."
"0",""
"0","x_train <- model.matrix(Salary ~ ., data = train_set %>% select(-split)) #dot in the formula means all available variables"
"0","x_valid <- model.matrix(Salary ~ ., data = validation_set %>% select(-split))[, -1]"
"0","#results below is a LIST"
"0","result <- glmnet(x = x_train[, -1], #predicted values of Salary, -1 mean we exclude the intercept column"
"0","                 y = train_set$Salary,"
"0","                 family = ""gaussian"", #instead of MSE we now using Normally Distributed Errors"
"0","                 alpha = 1, #selecting the LASSO penalty"
"0","                 lambda = 15 #penalty value"
"0","                 )"
"0",""
"0","rownames(coef(result))[which(coef(result) != 0)]"
"1"," [1]"
"1"," ""(Intercept)"""
"1"," ""Hits""       "
"1"," ""Runs""       "
"1"," ""Walks""      "
"1"," ""CHmRun""     "
"1"," ""CRuns""      "
"1"," ""CRBI""       "
"1"," ""DivisionW""  "
"1","
"
"1"," [9]"
"1"," ""PutOuts""    "
"1"," ""Assists""    "
"1"," ""NewLeagueN"" "
"1","
"
"0","#creating a true vs predicted plot"
"0",""
"0","y_pred <- as.numeric(predict(result, newx = x_valid))"
"0",""
"0","tibble(ObservedValue = validation_set$Salary,"
"0","       PredictedValue = y_pred) %>%"
"0","  ggplot(aes(x = PredictedValue, "
"0","             y = ObservedValue)) + "
"0","  geom_point() + "
"0","  geom_abline(slope = 1, intercept = 0, lty = 2)"
